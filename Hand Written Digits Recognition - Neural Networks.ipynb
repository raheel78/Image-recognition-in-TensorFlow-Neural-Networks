{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Written Digits Recognition using Convolutional Neural Networks\n",
    "\n",
    "This will be a step by step tutorial making use of jupyter notebook code cells to illustrate working of Google's TensorFlow library for the recognition of hand written images of digits. The data set was taken from MNIST which is well known source for these hand written images repository (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "The purpose of this notebook is to get some guided learning and note observations while applying nerual network learning mechanism using TensorFlow library.\n",
    "\n",
    "##### Some Primitive things to note down\n",
    "\n",
    "- The images are of hand written digits between 0 and 9 which puts the total possibilities as 10.\n",
    "- Images will also come with labels which are just mere markings of what each image is.\n",
    "- The data set will consists of following:\n",
    "    - Training Data Set\n",
    "    - Test Data Set\n",
    "    - Label Data Set\n",
    "    \n",
    "##### Hand Written Sketch (from MNIST)\n",
    "\n",
    "<img src=\"images/MNIST-Matrix.png\" style=\"width: 400px\" align=\"left\" /><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "##### Training Image Illustration\n",
    "\n",
    "<img src=\"images/mnist-train-xs.png\" style=\"width: 400px\" align=\"center\" /><br/><br/><br/>\n",
    "\n",
    "##### Label Vectors\n",
    "\n",
    "<img src=\"images/mnist-train-ys.png\" style=\"width: 400px\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Softmax Regression?\n",
    "\n",
    "Firstly, we need to understand what problem we are tyring to solve here?\n",
    "\n",
    "So, each image in this tutorial is any digit between 0 and 9 and we need to develop an algorithm which should be smart enough to look at the image and make some predictions about what might be that digit is. \n",
    "\n",
    "For example, pick an image of digit '9'. Now, the probability of sure evidence that this digit is actually nine would be 80%, while at the same time the probablity of taking it as digit '8' is 5% (since one loop above) and the rest of the probability can be anything which means that 15% chances of any other digit. \n",
    "\n",
    "- Probability of being the correct digit (i.e. '9') = 80%\n",
    "- Probability of digit read as eight ('8') = 5%\n",
    "- Probability for rest of anything else = 15% \n",
    "\n",
    "Remember that there is no sure shot rule of being a 100% correct guess.\n",
    "\n",
    "Softmax is a regression technique which is simple and fast enough to apply on types of problems like the one we are onto it. A softmax regression works in two steps:\n",
    "\n",
    "1. It will add up the evidence of our input (i.e. the image data) into the classes\n",
    "2. Later, it will convert this summed up evidence in to probabilities\n",
    "\n",
    "Now the question arises as how we will find out if our evidence is supporting for the image being in class and vice versa? We will do the pixel sum of weighted intensities and if the sum would be positive, that translates that our evidence is in the favor of the class. While, negative wieght will translate the fact that given input (image) is not the class.\n",
    "\n",
    "On top of that, we will add some bias ('b') to make sure that our evidence is independent of the provided input. \n",
    "\n",
    "Below is an illustration of images with two colors as <font color='red'>RED</font> and <font color='blue'>BLUE</font>. Blue marks the positive evidence while red marks the negative evidence in the image:\n",
    "\n",
    "<br/><img src=\"images/softmax-weights.png\" style=\"width: 270px\" align=\"center\" /><br/><br/>\n",
    "\n",
    "Moving forward, if we have an image called 'x' and we are hunting for the class 'i' then the equation for this evidence would be:\n",
    "\n",
    "\n",
    "$$ \\text{evidence}_i = \\sum_j W_{i,~ j} x_j + b_i $$\n",
    "\n",
    "where $W_i$ is the weights and $b_i$ is the bias for class $i$, and $j$ is an index for summing over the pixels in our input image $x$.\n",
    "\n",
    "Then as second step, we will convert our evidence into probablities $y$ using softmax function.\n",
    "\n",
    "$$ y = \\text{softmax}(\\text{evidence}) $$\n",
    "\n",
    "\n",
    "The output of this softmax function will be the probability distribution over 10 cases which means that the result will tell us what is the probability that the image in question represents which digit among 0 to 10.\n",
    "\n",
    "Above equation can be expanded to below form:\n",
    "\n",
    "$$ \\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "\n",
    "Let's take a more simpler approach to understand the working and outcome of Softmax function. What it does? The hypothesis coming out of this function SHOULD NOT have negative or zero weight. In order to achieve this, softmax function normalizes weight in a way that they could add up to one and this ensures a more formal & valid probability distribution.\n",
    "\n",
    "For each output, we added a weighted sum of $x$'s, add bias ($b$) and then apply softmax function. Following is an illustration for some sample outputs ($x$):\n",
    "\n",
    "<br/><img src=\"images/softmax-regression-scalargraph.png\" style=\"width: 370px\" align=\"center\" /><br/><br/>\n",
    "\n",
    "If I intend to convert above illustration as a mathematical equation, it would be;\n",
    "\n",
    "<br/><img src=\"images/softmax-regression-scalarequation.png\" style=\"width: 370px\" align=\"center\" /><br/>\n",
    "\n",
    "... and this equation can also be written in matrix vector form to make things more simple and easier to understand:\n",
    "\n",
    "<br/><img src=\"images/softmax-regression-vectorequation.png\" style=\"width: 370px\" align=\"center\" /><br/>\n",
    "\n",
    "And last (but not the least), the more compact form can be:\n",
    "\n",
    "$$ \\text{y} = softmax (W_x + b) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to Get the Hands Dirty with TensorFlow\n",
    "\n",
    "As we understand the concept and underlying mechanism as how prediction works with softmax regression as our technique, its time to see this theory in action. Let's give it a try and see how TensorFlow unfolds this mystery by implementing the regression.\n",
    "\n",
    "First step is to prepare this procedure in the form which TensorFlow can use. \n",
    "\n",
    "1. We will define a graph of interactions that tensor flow can use to do heavy numerical computations outside of Python. This will have place holders and variables, such as;\n",
    "\n",
    "- '$x$' will be place holder for 784 dimensional vector to hold input images.\n",
    "- '$W$' will be a variable matirx with modified weights having dimensions of 784 x 10.\n",
    "- '$b$' will be the biased factor to be added to evrey computation.  \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [none,784])\n",
    "\n",
    "W = tf.Variable(tf.zeros[784,10])\n",
    "\n",
    "b = tf.Variable(tf.zeros[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Why those dimensions?\n",
    "\n",
    "Before we proceed to define the model, let's understand why we have defined these dimensions as this is quite important to understand.\n",
    "\n",
    "$W$ which represents weight is a 784 by 10 dimensional matrix full of zeros. It is evident that since this $W$ will be multiplied to the image vector $x$ which has 784 dimensions so matrix multiplication will happen only when first vector ($x$ in this case) columns should be matching to the second vector ($W$ here) rows. The outcome will be a 10 dimensional vector of evidences. This resultant vector will then be added to vector $b$ which has got some dimensions as the evidence vector.\n",
    "\n",
    "Lastly, we will apply ```python softmax() ``` function from tensor flow library. Before this function is applied, we will use library function ```python tf.matmul(x,W) ``` to multiply two vectors and then add '$b$' as bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above statement was the final statement to define the model in tensor flow. Once defined, you can use this model on many devices and with many shapes and forms of data. As model is defined, now we will focus on training this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "Training the model is basically to identify how good (or bad) is our model data to be evaluated into something useful. In machine learning, we usually try to find out how far are we from getting good test results. In statistics, this is called \"cost\" or \"loss\" of a model. There are various techniques to calculate this loss, one of them well known function is cross-entropy. It is defined as;\n",
    "\n",
    "$$ H_{y'}(y) = -\\sum_i y'_i \\log(y_i)  $$\n",
    "\n",
    "where $y'$ is the predicted probability distribution, $y$ is the actual distribution (hot-vector of digital labels). To define the cross entropy in tensor flow, we first need a new place holder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Below we are implementing cross entropy function\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
